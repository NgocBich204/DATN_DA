import streamlit as st
import pandas as pd
import os
from datetime import datetime
def show():
    """Trang Import d·ªØ li·ªáu"""  
    st.title(" Import D·ªØ li·ªáu v√†o H·ªá th·ªëng")
    st.markdown("---")
    st.subheader("üìÇ B∆∞·ªõc 1: Ch·ªçn file Excel")
    uploaded_file = st.file_uploader(
        "Ch·ªçn file .xlsx ho·∫∑c .csv", 
        type=['xlsx', 'csv'],
        help="File ph·∫£i ch·ª©a c√°c c·ªôt: HoTen, Email, SDT, DonHang, NgayMua, SKU, TenSanPham, DoanhThuThuan..."
    )
    if uploaded_file:
        st.success(f" File: **{uploaded_file.name}** ({uploaded_file.size / 1024:.2f} KB)")
        try:
            if uploaded_file.name.endswith('.csv'):
                df_preview = pd.read_csv(uploaded_file, nrows=10)
            else:
                df_preview = pd.read_excel(uploaded_file, nrows=10)
            
            st.info(f"üìä S·ªë c·ªôt: **{len(df_preview.columns)}** | Preview 10 d√≤ng ƒë·∫ßu:")
            st.dataframe(df_preview, use_container_width=True)
            uploaded_file.seek(0)
        except Exception as e:
            st.error(f" L·ªói ƒë·ªçc file: {e}")
            return
        st.markdown("---")
        st.subheader("‚öôÔ∏è B∆∞·ªõc 2: Ch·ªçn ch·∫ø ƒë·ªô import")
        import_mode = st.radio(
            "Ch·ªçn c√°ch x·ª≠ l√Ω d·ªØ li·ªáu:",
            options=[
                "üóëÔ∏è X√≥a d·ªØ li·ªáu c≈© v√† thay th·∫ø ho√†n to√†n (Replace)",
                "‚ûï Ch·ªâ th√™m d·ªØ li·ªáu m·ªõi (Append)",
                "üîÑ C·∫≠p nh·∫≠t th√¥ng minh (Upsert)"
            ],
            help="""
            - Replace: X√≥a t·∫•t c·∫£ d·ªØ li·ªáu c≈©, import d·ªØ li·ªáu m·ªõi ho√†n to√†n
            - Append: Gi·ªØ d·ªØ li·ªáu c≈©, th√™m d·ªØ li·ªáu m·ªõi v√†o
            - Upsert: C·∫≠p nh·∫≠t n·∫øu ƒë√£ t·ªìn t·∫°i, th√™m m·ªõi n·∫øu ch∆∞a c√≥
            """
        )
        if "Replace" in import_mode:
            selected_mode = "replace"
            st.warning(" **C·∫£nh b√°o:** T·∫•t c·∫£ d·ªØ li·ªáu hi·ªán t·∫°i s·∫Ω b·ªã x√≥a!")
        elif "Append" in import_mode:
            selected_mode = "append"
            st.info("‚Ñπ D·ªØ li·ªáu m·ªõi s·∫Ω ƒë∆∞·ª£c th√™m v√†o, d·ªØ li·ªáu c≈© ƒë∆∞·ª£c gi·ªØ nguy√™n")
        else:
            selected_mode = "upsert"
            st.info("‚Ñπ D·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t th√¥ng minh (update + insert)")
        st.markdown("---")
        st.subheader(" B∆∞·ªõc 3: B·∫Øt ƒë·∫ßu import")
        col1, col2, col3 = st.columns([1, 1, 2])
        with col1:
            start_import = st.button(" B·∫Øt ƒë·∫ßu Import", type="primary", use_container_width=True)
        with col2:
            if st.button(" L√†m m·ªõi", use_container_width=True):
                st.rerun()
        if start_import:
            raw_folder = "raw_data"
            if not os.path.exists(raw_folder):
                os.makedirs(raw_folder)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_name = f"import_{timestamp}_{uploaded_file.name}"
            file_path = os.path.join(raw_folder, file_name)
            with open(file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            st.success(f" File ƒë√£ l∆∞u v√†o: `{file_path}`")
            from etl_core import ETLPipeline
            progress_container = st.container()
            status_text = st.empty()
            progress_bar = st.progress(0)
            try:
                status_text.info("üîß ƒêang kh·ªüi t·∫°o ETL Pipeline...")
                base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
                config_path = os.path.join(base_dir, "data_processing", "etl_config.json")
                if not os.path.exists(config_path):
                    st.error(f" Kh√¥ng t√¨m th·∫•y file config t·∫°i: {config_path}")
                    st.info(" ƒêang th·ª≠ t√¨m ·ªü v·ªã tr√≠ kh√°c...")
                    current_dir = os.path.dirname(os.path.abspath(__file__))
                    config_path = os.path.join(current_dir, "etl_config.json")
                    if not os.path.exists(config_path):
                        parent_dir = os.path.dirname(current_dir)
                        config_path = os.path.join(parent_dir, "etl_config.json")                   
                        if not os.path.exists(config_path):
                            st.error(" Kh√¥ng t√¨m th·∫•y file etl_config.json ·ªü b·∫•t k·ª≥ v·ªã tr√≠ n√†o!")
                            st.info(f" ƒê√£ t√¨m ki·∫øm t·∫°i:\n- {os.path.join(base_dir, 'data_processing', 'etl_config.json')}\n- {os.path.join(current_dir, 'etl_config.json')}\n- {os.path.join(parent_dir, 'etl_config.json')}")
                            st.stop()
                st.success(f" ƒê√£ t√¨m th·∫•y config: {config_path}")
                pipeline = ETLPipeline(config_path=config_path)
                progress_bar.progress(5)
                status_text.info("üîó ƒêang k·∫øt n·ªëi database...")
                success, msg = pipeline.connect_db()
                if not success:
                    st.error(msg)
                    st.stop()
                st.success(msg)
                progress_bar.progress(10)
                status_text.info(" ƒêang ƒë·ªçc v√† l√†m s·∫°ch d·ªØ li·ªáu...")
                success, msg, df = pipeline.read_and_clean_excel(file_path)
                if not success:
                    st.error(msg)
                    pipeline.close_db()
                    st.stop()
                st.success(msg)
                progress_bar.progress(20)    
                status_text.info(" ƒêang load d·ªØ li·ªáu v√†o Staging table...")
                success, msg = pipeline.load_to_staging(mode=selected_mode)
                if not success:
                    st.error(msg)
                    pipeline.close_db()
                    st.stop()
                st.success(msg)
                progress_bar.progress(35)
                status_text.info("ƒêang t·∫°o Dimension tables...")
                success, msg = pipeline.create_dimension_tables(mode=selected_mode)
                if not success:
                    st.error(msg)
                    pipeline.close_db()
                    st.stop()
                st.success(msg)
                progress_bar.progress(50)
                status_text.info(" ƒêang t·∫°o Fact table...")
                success, msg = pipeline.create_fact_table()
                if not success:
                    st.error(msg)
                    pipeline.close_db()
                    st.stop()
                st.success(msg)
                progress_bar.progress(65)
                status_text.info(" ƒêang t√≠nh to√°n RFM...")
                success, msg, df_rfm = pipeline.calculate_rfm()
                if not success:
                    st.error(msg)
                    pipeline.close_db()
                    st.stop()
                st.success(msg)
                progress_bar.progress(80)
                # B∆∞·ªõc 7: Ph√¢n c·ª•m KMeans
                status_text.info(" ƒêang ph√¢n c·ª•m kh√°ch h√†ng v·ªõi KMeans...")
                success, msg, segment_stats = pipeline.kmeans_clustering(df_rfm, auto_k=True)
                if not success:
                    st.error(msg)
                    pipeline.close_db()
                    st.stop()
                st.success(msg)
                progress_bar.progress(95)
                # ƒê√≥ng k·∫øt n·ªëi
                pipeline.close_db()
                progress_bar.progress(100)
                status_text.success(" **Import ho√†n th√†nh!**")
                st.balloons()
                st.markdown("---")
                st.subheader(" K·∫øt qu·∫£ Import")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric(
                        label=" S·ªë d√≤ng d·ªØ li·ªáu", 
                        value=f"{len(df):,}"
                    )
                with col2:
                    st.metric(
                        label=" S·ªë kh√°ch h√†ng", 
                        value=f"{len(df_rfm):,}"
                    )
                with col3:
                    st.metric(
                        label=" S·ªë ph√¢n kh√∫c", 
                        value=pipeline.optimal_k
                    )
                
                # Ph√¢n b·ªï ph√¢n kh√∫c
                st.markdown("---")
                st.markdown("###  Ph√¢n b·ªï kh√°ch h√†ng theo ph√¢n kh√∫c:")
                
                if segment_stats:
                    for segment_name, count in segment_stats.items():
                        st.markdown(f"- **{segment_name}**: {count:,} kh√°ch h√†ng")
                
                # N√∫t chuy·ªÉn trang
                st.markdown("---")
                col1, col2 = st.columns(2)
                with col1:
                    if st.button("üìäXem Dashboard", type="primary", use_container_width=True):
                        st.session_state.page = "T·ªïng qu√°t"
                        st.rerun()
                
                with col2:
                    if st.button("üîç Tra c·ª©u kh√°ch h√†ng", use_container_width=True):
                        st.session_state.page = "Tra c·ª©u"
                        st.rerun()
                
            except Exception as e:
                st.error(f"L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω: {str(e)}")
                st.exception(e)
                if 'pipeline' in locals():
                    pipeline.close_db()
    else:
        st.info(" Vui l√≤ng upload file Excel ƒë·ªÉ b·∫Øt ƒë·∫ßu")